import requests
import json
import openai
import os
import time
import feedparser
import uuid
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from datetime import datetime, timedelta
from dateutil.parser import parse as date_parse
from dateutil import parser as date_parser
import pytz




# Carregar variáveis de ambiente
load_dotenv()

# Carregar chave da API OpenAI
openai.api_key = os.getenv('OPENAI_API_KEY')

# Carregar dados e títulos processados de arquivo JSON
json_filename = 'data.json'
if os.path.exists(json_filename):
    with open(json_filename, 'r') as json_file:
        data = json.load(json_file)
        processed_titles = [item['title'] for item in data]
else:
    data = []
    processed_titles = []

# Função para salvar dados em arquivo JSON
def save_data(data):
    with open(json_filename, 'w') as json_file:
        json.dump(data, json_file)

# Lista de URLs dos feeds RSS
url_list = [
    'https://g1.globo.com/rss/g1/economia',
    'https://g1.globo.com/rss/g1/tecnologia',
    'https://g1.globo.com/rss/g1/ciencia-e-saude',
    'https://g1.globo.com/rss/g1/educacao',
    'https://g1.globo.com/rss/g1/mundo',
    'http://pox.globo.com/rss/valor',
    'https://agenciabrasil.ebc.com.br/rss/saude/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/economia/feed.xml'
    'https://agenciabrasil.ebc.com.br/rss/educacao/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/esportes/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/ultimasnoticias/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/geral/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/internacional/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/justica/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/politica/feed.xml',
    'https://noticias.r7.com/saude/feed.xml',
    'https://noticias.r7.com/tecnologia-e-ciencia/feed.xml',
    'https://noticias.r7.com/economia/feed.xml',
    'https://noticias.r7.com/educacao/feed.xml',
    'https://noticias.r7.com/concursos/feed.xml',
    'https://noticias.r7.com/politica/feed.xml',
    'https://noticias.r7.com/internacional/feed.xml'
    'https://esportes.r7.com/feed.xml',
    'https://esportes.r7.com/e-sports/feed.xml',
    'https://esportes.r7.com/lance/feed.xml',
    'https://esportes.r7.com/mais-esportes/feed.xml',
    'https://esportes.r7.com/futebol/feed.xml',
    'https://lifestyle.r7.com/feed.xml',
    'https://lifestyle.r7.com/bem-estar/feed.xml'

]

# Mapa de categorias por URL
categorias_por_url = {
    "https://g1.globo.com/rss/g1/economia": 34,
    "https://g1.globo.com/rss/g1/tecnologia": 57,
    "https://g1.globo.com/rss/g1/ciencia-e-saude": 9,
    "https://g1.globo.com/rss/g1/educacao": 35,
    "https://g1.globo.com/rss/g1/mundo": 39,
    "http://pox.globo.com/rss/valor": 20,
    "https://agenciabrasil.ebc.com.br/rss/saude/feed.xml": 58,
    "https://agenciabrasil.ebc.com.br/rss/economia/feed.xml": 34,
    "https://agenciabrasil.ebc.com.br/rss/educacao/feed.xml": 35,
    "https://agenciabrasil.ebc.com.br/rss/esportes/feed.xml": 60,
    "https://agenciabrasil.ebc.com.br/rss/ultimasnoticias/feed.xml": 29,
    "https://agenciabrasil.ebc.com.br/rss/geral/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/internacional/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/justica/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/politica/feed.xml": 31,
    "https://noticias.r7.com/saude/feed.xml": 58,
    "https://noticias.r7.com/tecnologia-e-ciencia/feed.xml": 57,
    "https://noticias.r7.com/economia/feed.xml": 34,
    "https://noticias.r7.com/educacao/feed.xml": 35,
    "https://noticias.r7.com/concursos/feed.xml": 30,
    "https://noticias.r7.com/politica/feed.xml": 29,
    "https://noticias.r7.com/internacional/feed.xml": 39,
    "https://esportes.r7.com/feed.xml": 60,
    "https://esportes.r7.com/e-sports/feed.xml": 60,
    "https://esportes.r7.com/lance/feed.xml": 60,
    "https://esportes.r7.com/mais-esportes/feed.xml": 60,
    "https://esportes.r7.com/futebol/feed.xml": 40,
    "https://lifestyle.r7.com/feed.xml": 59,
    "https://lifestyle.r7.com/bem-estar/feed.xml": 59

}


# Configuração das credenciais e cabeçalhos de autenticação da API WordPress - testes
#api_url = "https://girodiario.com.br/wordpress/wp-json/wp/v2"

# Configuração das credenciais e cabeçalhos de autenticação da API WordPress - producao
api_url = "https://girodiario.com.br/wp-json/wp/v2"

# Credenciais de autenticação [ambiente producao]
usuario = "GD"
senha = "Kb3tUsHjo0DrbzyRdEB7lLp9"

# Credenciais de autenticação [ambiente teste]
#senha = aU^$ccG$S%p!^iI(FgodKcB#



# Loop sobre a lista de URLs do feed RSS
for url in url_list:
    response = requests.get(url)
    feed = feedparser.parse(response.content)

    num_entries = len(feed.entries)
    description_count = 0
    link_count = 0
    title_count = 0
    media_content_count = 0
    mediaurl_content_count = 0
    imagem_destaque_count = 0

    # Ignorar o primeiro artigo de cada URL
    num_entries = len(feed.entries) - 1

    for entry in feed.entries:
        if 'description' in entry:
            description_count += 1
        if 'link' in entry:
            link_count += 1
        if 'title' in entry:
            title_count += 1
        if 'media:content' in entry:
            media_content_count += 1
        if 'mediaurl' in entry:
            mediaurl_content_count += 1
        if 'imagem-destaque' in entry:
            imagem_destaque_count += 1

        # Parsing de diferentes formatos de data
        if 'updated' in entry:
            updated = date_parser.parse(entry.updated).strftime('%Y-%m-%d %H:%M:%S')
        elif 'pubDate' in entry:
            pubdate = date_parser.parse(entry.pubDate).strftime('%Y-%m-%d %H:%M:%S')


    # Pausa de 2 segundos após processar cada feed
    time.sleep(2)

    # Definir data limite (30 dias atrás)
    date_limit = datetime.now(pytz.utc) - timedelta(days=5)

    for entry in feed.entries[:50]:
        # Verificar data de publicação do artigo
        updated = entry.get('updated')
        if updated:
            pub_date = date_parse(updated)
        else:
            pub_date = date_parser.parse(entry.published)
        if pub_date < date_limit:
            print(f"Skipping article '{entry.title}' published on {entry.published}")
            continue

        title = entry.title
        link = entry.link

        # Verificar se o título já foi processado
        if title in processed_titles:
            print(f"Title '{title}' already processed. Skipping.")
            continue

        # Adicionar título à lista de títulos processados
        processed_titles.append(title)


        # Iniciar img_path como None
        img_path = None

        # Extrair URL da imagem em destaque ebc
        if 'imagem-destaque' in entry:
            img_path = entry['imagem-destaque']

            # Extrair o caminho da imagem começando com "agenciabrasil.ebc.com.br" e terminando com ".jpg" ou ".png"
            import re
            pattern = r"(agenciabrasil\.ebc\.com\.br[^.]+(\.jpg|\.png))"
            match = re.search(pattern, img_path)
            if match:
                img_path = 'https://' + match.group(1)
            else:
                img_path = None

        # Extrair URL da imagem em destaque r7
        elif 'mediaurl' in entry:
            img_url = entry['mediaurl']

            if img_url:
                img_path = img_url
            else:
                img_path = None

        # Extrair URL da imagem em destaque g1, valor
        elif 'media_content' in entry: # g1
            for media in entry.media_content:
                if media.get('medium') == 'image':
                    img_path = media.get('url')
                    break
            else:
                img_path = None

        # Imprimir a URL da imagem gerada
        print(img_path)





        # Extrair conteúdo do artigo usando BeautifulSoup
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')
        article_content = soup.find('div', class_='node-content')
        if article_content:
            article_text = article_content.text
        else:
            article_text = ""

        # Reescrever título e conteúdo usando OpenAI GPT-4
        if title:
            print(f"Title before rewriting: {title}")
            print("Aguardando Retorno OpenAI")
            #title_prompt = f"Reescreva o título com no máximo duas linhas:\n\n{title}"
            title_prompt = f"Reescreva o título do artigo a seguir mais envolvente e digno de clique, com no maximo 2 linhas curtas:\n\n{title}"

            try:
                title_response = openai.Completion.create(engine="text-davinci-003", prompt=title_prompt, max_tokens=50, n=1, stop=None, temperature=0.5)
                time.sleep(10)  # Espera 5 segundos antes de fazer a próxima chamada
                if title_response.choices[0].text:
                    new_title = title_response.choices[0].text.strip()
                else:
                    new_title = title
                print(f"Novo título: {new_title}")
            except openai.error.APIError as e:
                print(f"Erro ao obter novo título: {e}")
                time.sleep(30)
                title_response = openai.Completion.create(engine="text-davinci-003", prompt=title_prompt, max_tokens=50, n=1, stop=None, temperature=0.5)
                if title_response.choices[0].text:
                    new_title = title_response.choices[0].text.strip()
                else:
                    new_title = title
                print(f"Novo título: {new_title}")

            content_prompt = f"Por favor, leia os dados da url enviada e compreenda o assunto discutido em detalhes. Reescreva o texto de maneira mais detalhada e única, evitando a repetição de parágrafos e garantindo que o conteúdo seja relevante para o tema. Utilize elementos de formatação, como marcadores, listas numeradas, negrito e itálico. Além disso, organize o texto de acordo com as melhores práticas de SEO e divida-o em parágrafos separados. Siga a estrutura a seguir ao criar o artigo: 1. sempre Inicie o texto com um parágrafo introdutório (sem a tag H1). 2. Insira um subtítulo H2. 3. Adicione um novo parágrafo após o subtítulo H2. 4. Continue alternando entre subtítulos (H2, H3 ou H4) e parágrafos conforme necessário, garantindo que cada seção seja única e não repetitiva. 5. Certifique-se de incluir uma conclusão no final do artigo. Verifique a ortografia. Formate somente os subtítulos com h2, h3, h4 e entregue o texto formatado em HTML:\n\n{link}"
            try:
                content_response = openai.Completion.create(engine="text-davinci-003", prompt=content_prompt, max_tokens=2048, n=1, stop=None, temperature=0.5)
                time.sleep(10)  # Espera 5 segundos antes de fazer a próxima chamada
                if content_response.choices[0].text:
                    new_content = content_response.choices[0].text.strip()
                else:
                    new_content = ""
                print(f"Novo conteúdo: {new_content[:100]}")
            except openai.error.APIError as e:
                print(f"Erro ao obter novo conteúdo: {e}")
                time.sleep(30)
                content_response = openai.Completion.create(engine="text-davinci-003", prompt=content_prompt, max_tokens=2048, n=1, stop=None, temperature=0.5)
                if content_response.choices[0].text:
                    new_content = content_response.choices[0].text.strip()
                    if new_content.startswith("<p>.</p>"):
                        new_content = new_content.replace("<p>.</p>", "", 1)
                else:
                    new_content = ""
                print(f"Novo conteúdo: {new_content[:100]}")

            # Adicionar tags HTML ao novo conteúdo
            #new_content = f'<p><img src="{img_path}" /></p><p>{new_content}</p>'
            #print(f"Novo título: {new_title}")
            #new_content = new_content.replace("<p>.</p>", "", 1)
            #print(f"Novo conteúdo: {new_content[:100]}")


            # Gerando um nome de arquivo aleatório
            imagem_destacada_nome = f"{uuid.uuid4()}.jpg"


            # Verificar se há uma imagem em destaque
            print(img_path)
            if img_path:
                # Gerar um nome de arquivo aleatório
                imagem_destacada_nome = f"{uuid.uuid4()}.jpg"

                # Configurar o cabeçalho da requisição para a criação da imagem destacada
                headers = {
                    "Content-Disposition": f"attachment; filename={imagem_destacada_nome}",
                    "Content-Type": "image/jpeg",
                }


                # Requisição para criar a imagem destacada
                try:
                    print(f"Enviando imagem '{img_path[:100]}' para o WordPress...")
                    imagem_destacada_resposta = requests.post(
                        api_url + "/media",
                        auth=(usuario, senha),
                        headers=headers,
                        data=requests.get(img_path).content
                    )
                    imagem_destacada_resposta.raise_for_status()
                    imagem_destacada_resposta_json = imagem_destacada_resposta.json()

                    if "id" in imagem_destacada_resposta_json:
                        imagem_destacada_id = imagem_destacada_resposta_json["id"]
                        print(f"Arquivo de imagem baixado: {imagem_destacada_nome[:100]}")
                    else:
                        print("Erro ao criar imagem destacada")
                        print(imagem_destacada_resposta_json)
                        exit()
                except requests.exceptions.RequestException as e:
                    print("Erro ao criar imagem destacada")
                    print(f"Exception: {e}")
                    exit()

            else:
                # Caso não haja imagem em destaque, definir imagem_destacada_id como None
                imagem_destacada_id = None



            # Requisição POST para criar novo post no blog WordPress
            headers = {"Content-Type": "application/json"}
            categoria_id = categorias_por_url[url]
            if imagem_destacada_id:
                dados = {
                    "title": new_title,
                    "content": new_content,
                    "featured_media": imagem_destacada_id,
                    "status": "publish",
                    "comment_status": "closed",
                    "ping_status": "closed",
                    "categories": [categoria_id],
                }
            else:
                dados = {
                    "title": new_title,
                    "content": new_content,
                    "status": "draft",
                    "comment_status": "closed",
                    "ping_status": "closed",
                    "categories": [categoria_id],
                    "featured_media": 0
                }
            post_resposta = requests.post(api_url + "/posts?_embed", auth=(usuario, senha), headers=headers, json=dados)

            # Verificar se a requisição foi bem-sucedida
            if post_resposta.status_code == 201:
                print("Artigo enviado com sucesso ao blog!")
            else:
                print(f"Erro ao enviar o artigo: {post_resposta.content}")

            # Adicionar item processado à lista de dados
            data.append({
                'title': title,
                'link': link,
                'processed_at': str(datetime.now())
            })

            # Salvar dados no arquivo JSON
            save_data(data)
