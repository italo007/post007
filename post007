
import requests
import json
import openai
import os
import time
import feedparser
#import uuid
import string
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from datetime import datetime, timedelta
from dateutil.parser import parse as date_parse
from dateutil import parser as date_parser
import pytz
from PIL import Image
import io



# Carregar variáveis de ambiente
load_dotenv()

# Carregar chave da API OpenAI
openai.api_key = os.getenv('OPENAI_API_KEY')

# Carregar dados e títulos processados de arquivo JSON
json_filename = 'data.json'
if os.path.exists(json_filename):
    with open(json_filename, 'r') as json_file:
        data = json.load(json_file)
        processed_titles = [item['title'] for item in data]
else:
    data = []
    processed_titles = []

# Função para salvar dados em arquivo JSON
def save_data(data):
    with open(json_filename, 'w') as json_file:
        json.dump(data, json_file)

# Lista de URLs dos feeds RSS
url_list = [
    'https://g1.globo.com/rss/g1/economia',
    'https://g1.globo.com/rss/g1/tecnologia',
    'https://g1.globo.com/rss/g1/ciencia-e-saude',
    'https://g1.globo.com/rss/g1/educacao',
    'https://g1.globo.com/rss/g1/mundo',
    'http://pox.globo.com/rss/valor',
    'https://g1.globo.com/rss/g1/sp/bauru-marilia/',
    'https://g1.globo.com/rss/g1/sp/campinas-regiao/',
    'https://g1.globo.com/rss/g1/sao-paulo/itapetininga-regiao/',
    'https://g1.globo.com/rss/g1/sp/mogi-das-cruzes-suzano/',
    'https://g1.globo.com/rss/g1/sp/piracicaba-regiao/',
    'https://g1.globo.com/rss/g1/sp/presidente-prudente-regiao/',
    'https://g1.globo.com/rss/g1/sp/ribeirao-preto-franca/',
    'https://g1.globo.com/rss/g1/sp/santos-regiao/',
    'https://g1.globo.com/rss/g1/sp/sao-carlos-regiao/',
    'https://g1.globo.com/rss/g1/sp/vale-do-paraiba-regiao/'
    'https://agenciabrasil.ebc.com.br/rss/saude/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/economia/feed.xml'
    'https://agenciabrasil.ebc.com.br/rss/educacao/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/esportes/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/ultimasnoticias/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/geral/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/internacional/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/justica/feed.xml',
    'https://agenciabrasil.ebc.com.br/rss/politica/feed.xml',
    'https://noticias.r7.com/saude/feed.xml',
    'https://noticias.r7.com/tecnologia-e-ciencia/feed.xml',
    'https://noticias.r7.com/economia/feed.xml',
    'https://noticias.r7.com/educacao/feed.xml',
    'https://noticias.r7.com/concursos/feed.xml',
    'https://noticias.r7.com/politica/feed.xml',
    'https://noticias.r7.com/internacional/feed.xml'
    'https://esportes.r7.com/feed.xml',
    'https://esportes.r7.com/e-sports/feed.xml',
    'https://esportes.r7.com/lance/feed.xml',
    'https://esportes.r7.com/mais-esportes/feed.xml',
    'https://esportes.r7.com/futebol/feed.xml',
    'https://lifestyle.r7.com/feed.xml',
    'https://lifestyle.r7.com/bem-estar/feed.xml'

]

# Mapa de categorias por URL
categorias_por_url = {
    "https://g1.globo.com/rss/g1/economia": 34,
    "https://g1.globo.com/rss/g1/tecnologia": 57,
    "https://g1.globo.com/rss/g1/ciencia-e-saude": 9,
    "https://g1.globo.com/rss/g1/educacao": 35,
    "https://g1.globo.com/rss/g1/mundo": 39,
    "http://pox.globo.com/rss/valor": 20,
    "https://g1.globo.com/rss/g1/sp/bauru-marilia/": 36,
    "https://g1.globo.com/rss/g1/sp/campinas-regiao/": 36,
    "https://g1.globo.com/rss/g1/sao-paulo/itapetininga-regiao/": 36,
    "https://g1.globo.com/rss/g1/sp/mogi-das-cruzes-suzano/": 36,
    "https://g1.globo.com/rss/g1/sp/piracicaba-regiao/": 36,
    "https://g1.globo.com/rss/g1/sp/presidente-prudente-regiao/": 36,
    "https://g1.globo.com/rss/g1/sp/ribeirao-preto-franca/": 36,
    "https://g1.globo.com/rss/g1/sp/santos-regiao/": 36,
    "https://g1.globo.com/rss/g1/sp/sao-carlos-regiao/": 36,
    "https://g1.globo.com/rss/g1/sp/vale-do-paraiba-regiao/": 36,
    "https://agenciabrasil.ebc.com.br/rss/saude/feed.xml": 58,
    "https://agenciabrasil.ebc.com.br/rss/economia/feed.xml": 34,
    "https://agenciabrasil.ebc.com.br/rss/educacao/feed.xml": 35,
    "https://agenciabrasil.ebc.com.br/rss/esportes/feed.xml": 60,
    "https://agenciabrasil.ebc.com.br/rss/ultimasnoticias/feed.xml": 29,
    "https://agenciabrasil.ebc.com.br/rss/geral/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/internacional/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/justica/feed.xml": 31,
    "https://agenciabrasil.ebc.com.br/rss/politica/feed.xml": 31,
    "https://noticias.r7.com/saude/feed.xml": 58,
    "https://noticias.r7.com/tecnologia-e-ciencia/feed.xml": 57,
    "https://noticias.r7.com/economia/feed.xml": 34,
    "https://noticias.r7.com/educacao/feed.xml": 35,
    "https://noticias.r7.com/concursos/feed.xml": 30,
    "https://noticias.r7.com/politica/feed.xml": 29,
    "https://noticias.r7.com/internacional/feed.xml": 39,
    "https://esportes.r7.com/feed.xml": 60,
    "https://esportes.r7.com/e-sports/feed.xml": 60,
    "https://esportes.r7.com/lance/feed.xml": 60,
    "https://esportes.r7.com/mais-esportes/feed.xml": 60,
    "https://esportes.r7.com/futebol/feed.xml": 40,
    "https://lifestyle.r7.com/feed.xml": 59,
    "https://lifestyle.r7.com/bem-estar/feed.xml": 59

}


# Configuração das credenciais e cabeçalhos de autenticação da API WordPress - testes
#api_url = "https://girodiario.com.br/wordpress/wp-json/wp/v2"

# Configuração das credenciais e cabeçalhos de autenticação da API WordPress - producao
api_url = "https://girodiario.com.br/wp-json/wp/v2"

# Credenciais de autenticação [ambiente producao]
usuario = "GD"
senha = "Kb3tUsHjo0DrbzyRdEB7lLp9"

# Credenciais de autenticação [ambiente teste]
#senha = aU^$ccG$S%p!^iI(FgodKcB#



# Loop sobre a lista de URLs do feed RSS
for url in url_list:
    response = requests.get(url)
    feed = feedparser.parse(response.content)

    num_entries = len(feed.entries)
    description_count = 0
    link_count = 0
    title_count = 0
    media_content_count = 0
    mediaurl_content_count = 0
    imagem_destaque_count = 0

    # Ignorar o primeiro artigo de cada URL
    num_entries = len(feed.entries) - 1

    for entry in feed.entries:
        if 'description' in entry:
            description_count += 1
        if 'link' in entry:
            link_count += 1
        if 'title' in entry:
            title_count += 1
        if 'media:content' in entry:
            media_content_count += 1
        if 'mediaurl' in entry:
            mediaurl_content_count += 1
        if 'imagem-destaque' in entry:
            imagem_destaque_count += 1

        # Parsing de diferentes formatos de data
        if 'updated' in entry:
            updated = date_parser.parse(entry.updated).strftime('%Y-%m-%d %H:%M:%S')
        elif 'pubDate' in entry:
            pubdate = date_parser.parse(entry.pubDate).strftime('%Y-%m-%d %H:%M:%S')


    # Pausa de 2 segundos após processar cada feed
    time.sleep(2)

    # Definir data limite (30 dias atrás)
    date_limit = datetime.now(pytz.utc) - timedelta(days=1)

    for entry in feed.entries[:30]:
        # Verificar data de publicação do artigo
        updated = entry.get('updated')
        if updated:
            pub_date = date_parse(updated)
        else:
            pub_date = date_parser.parse(entry.published)
        if pub_date < date_limit:
            print(f"Skipping article '{entry.title}' published on {entry.published}")
            continue

        title = entry.title
        link = entry.link

        # Verificar se o título já foi processado
        if title in processed_titles:
            print(f"Title '{title}' already processed. Skipping.")
            continue

        # Adicionar título à lista de títulos processados
        processed_titles.append(title)


        # Iniciar img_path como None
        img_path = None

        # Extrair URL da imagem em destaque ebc
        if 'imagem-destaque' in entry:
            img_path_ebc = entry['imagem-destaque']

            # Extrair o caminho da imagem começando com "agenciabrasil.ebc.com.br" e terminando com ".jpg" ou ".png"
            import re
            pattern = r"(agenciabrasil\.ebc\.com\.br[^.]+(\.jpg|\.png))"
            match = re.search(pattern, img_path_ebc)
            if match:
                img_path = 'https://' + match.group(1)

        # Extrair URL da imagem em destaque r7
        elif 'mediaurl' in entry:
            img_url = entry['mediaurl']

            if img_url:
                img_path = img_url

        # Extrair URL da imagem em destaque g1, valor
        elif 'media_content' in entry: # g1
            for media in entry.media_content:
                if media.get('medium') == 'image':
                    img_path = media.get('url')
                    break

        # Imprimir a URL da imagem gerada
        print(img_path)

        # Verificar se a URL da imagem responde
        if img_path:
            try:
                response = requests.get(img_path)
                response.raise_for_status()
            except Exception as err:
                print(f"Erro ao acessar a URL da imagem: {err}")
                img_path = None
                print(img_path)




        # Extrair conteúdo do artigo usando BeautifulSoup
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')
        article_content = soup.find('div', class_='node-content')
        if article_content:
            article_text = article_content.text
        else:
            article_text = ""

        # Reescrever título usando OpenAI GPT-4
        if title:
            print(f"Title before rewriting: {title}")
            print("Aguardando Retorno OpenAI")
            #title_prompt = f"Reescreva o título com no máximo duas linhas:\n\n{title}"
            title_prompt = f"Reescreva o título do artigo a seguir mais envolvente e digno de clique, com no maximo 2 linhas curtas:\n\n{title}"
            try:
                title_response = openai.Completion.create(engine="text-davinci-003", prompt=title_prompt, max_tokens=50, n=1, stop=None, temperature=0.6)
            except (openai.error.APIError, openai.error.RateLimitError, openai.error.APIConnectionError) as e:
                print(f"Erro ao obter novo conteúdo: {e}")
                if isinstance(e, openai.error.RateLimitError):
                    print("Aguardando 5 minutos antes de tentar novamente...")
                    time.sleep(300)  # Aguardar 5 minutos (300 segundos)
                else:
                    print("Aguardando 1 minuto antes de tentar novamente...")
                    time.sleep(90)
                title_response = openai.Completion.create(engine="text-davinci-003", prompt=title_prompt, max_tokens=50, n=1, stop=None, temperature=0.6)
                if title_response.choices[0].text:
                    new_title = title_response.choices[0].text.strip()
                else:
                    new_title = title
                print(f"Novo título: {new_title}")

        # Reescrever conteúdo usando OpenAI GPT-4

            #content_prompt = f"Por favor, leia os dados da url enviada e compreenda o assunto discutido em detalhes. Reescreva o texto de maneira mais detalhada e única, evitando a repetição de parágrafos e garantindo que o conteúdo seja relevante para o tema. Utilize elementos de formatação, como marcadores, listas numeradas, negrito e itálico. Além disso, organize o texto de acordo com as melhores práticas de SEO e divida-o em parágrafos separados. Siga a estrutura a seguir ao criar o artigo: 1. sempre Inicie o texto com um parágrafo introdutório entre 120 a 130 palavras (sem a tag H1). 2. Insira um subtítulo H2. 3. Adicione um novo parágrafo relacionado o subtítulo H2 com 120 a 130 palavras. 4. Continue alternando entre subtítulos (H2, H3 ou H4) e parágrafos entre 120 a 130 palavras, garantindo que cada seção seja única e não repetitiva. 5. Certifique-se de incluir uma conclusão no final do artigo entre 120 a 130 palavras. Verifique a ortografia. Formate somente os subtítulos com h2, h3, h4 e entregue o texto formatado em HTML:\n\n{link}"
            #content_prompt = f"Por favor, leia os dados da url enviada. Reescreva todo o texto deste artigo com outras palavras, n evitando a repetição de parágrafos e garantindo que o conteúdo seja relevante para o tema. Utilize elementos de formatação, como marcadores, listas numeradas, negrito e itálico. Além disso, organize o texto de acordo com as melhores práticas de SEO e divida-o em parágrafos separados. Siga a estrutura a seguir ao criar o artigo: 1. sempre Inicie o texto com um parágrafo introdutório entre 120 a 130 palavras (sem a tag H1). 2. Insira um subtítulo H2. 3. Adicione um novo parágrafo relacionado o subtítulo H2 com 120 a 130 palavras. 4. Continue alternando entre subtítulos (H2, H3 ou H4) e parágrafos entre 120 a 130 palavras, garantindo que cada seção seja única e não repetitiva. 5. Certifique-se de incluir uma conclusão no final do artigo entre 120 a 130 palavras. Verifique a ortografia. Formate somente os subtítulos com h2, h3, h4 e entregue o texto formatado em HTML:\n\n{link}"
            content_prompt = "Por favor, leia os dados da URL enviada e reescreva o texto com suas próprias palavras, mantendo-se fiel às informações fornecidas. Utilize elementos de formatação, como marcadores, listas numeradas, negrito e itálico, quando aplicável. Organize o texto de acordo com as melhores práticas de SEO e divida-o em parágrafos separados. Siga a estrutura a seguir ao criar o artigo:\n\n1. Inicie o texto com um parágrafo introdutório entre 120 a 130 palavras (sem a tag H1).\n2. Insira um subtítulo H2.\n3. Adicione um novo parágrafo relacionado ao subtítulo H2, com 120 a 130 palavras.\n4. Continue alternando entre subtítulos (H2, H3 ou H4) e parágrafos de 120 a 130 palavras, garantindo que cada seção seja única e não repetitiva.\n5. Certifique-se de incluir uma conclusão no final do artigo entre 120 a 130 palavras. Verifique a ortografia.\n\nPor favor, verifique a precisão das informações durante a reescrita, evitando adicionar ou inventar dados não presentes no texto original. Formate apenas os subtítulos com h2, h3, h4 e entregue o texto formatado em HTML:\n\n{link}"
            try:
                content_response = openai.Completion.create(engine="text-davinci-003", prompt=content_prompt, max_tokens=3496, n=1, stop=None, temperature=0.6)
                time.sleep(15)  # Espera 15 segundos antes de fazer a próxima chamada
                if content_response.choices[0].text:
                    new_content = content_response.choices[0].text.strip()
                else:
                    new_content = ""
                print(f"Novo conteúdo: {new_content[:100]}")
            except (openai.error.APIError, openai.error.RateLimitError, openai.error.APIConnectionError) as e:
                print(f"Erro ao obter novo conteúdo: {e}")
                if isinstance(e, openai.error.RateLimitError):
                    print("Aguardando 5 minutos antes de tentar novamente...")
                    time.sleep(300)  # Aguardar 5 minutos (300 segundos)
                else:
                    print("Aguardando 1 minuto antes de tentar novamente...")
                    time.sleep(90)
                content_response = openai.Completion.create(engine="text-davinci-003", prompt=content_prompt, max_tokens=3596, n=1, stop=None, temperature=0.6)
                if content_response.choices[0].text:
                    new_content = content_response.choices[0].text.strip()
                    if new_content.startswith("<p>.</p>"):
                        new_content = new_content.replace("<p>.</p>", "", 1)
                else:
                    new_content = ""
                print(f"Novo conteúdo: {new_content[:100]}")

            # Adicionar tags HTML ao novo conteúdo
            #new_content = f'<p><img src="{img_path}" /></p><p>{new_content}</p>'
            #print(f"Novo título: {new_title}")
            new_content = new_content.replace("<p>.html</p>", "", 1)
            #print(f"Novo conteúdo: {new_content[:100]}")





            # Gerando um nome de arquivo aleatório
            #imagem_destacada_nome = f"{uuid.uuid4()}.jpg"

            new_title = title  # Inicializar a variável new_title


            # Verificar se há uma imagem em destaque nome da imagem novo
            print(img_path)
            if img_path:
                # Gerar nome de arquivo com as duas primeiras palavras do título e extensão .webp
                translator = str.maketrans("", "", string.punctuation + "/.")
                new_title_clean = new_title.translate(translator).lower().replace(' ', '-')
                new_title_words = new_title_clean.split("-")[:12]
                new_title_clean = "-".join(new_title_words)
                imagem_destacada_nome = f"{new_title_clean}.webp".replace("\n", " ")

                # Configurar o cabeçalho da requisição para a criação da imagem destacada
                headers = {
                    "Content-Disposition": f"attachment; filename={imagem_destacada_nome.encode('utf-8').decode('latin-1')}",
                    "Content-Type": "image/webp",
                }

                # Realizar o download da imagem da URL
                response = requests.get(img_path)
                response.raise_for_status()


                # Carregar a imagem
                original_image = Image.open(io.BytesIO(response.content))

                # Obter o tamanho original da imagem
                original_size = len(response.content)
                print(f"Tamanho original da imagem: {original_size} bytes")

                # Obter a resolução da imagem original
                original_width, original_height = original_image.size
                print(f"Resolução da imagem original: {original_width} x {original_height} pixels")


                # Obter as dimensões da imagem
                width, height = original_image.size

                # Definir as dimensões para os formatos landscape e square
                landscape_width, landscape_height = 1200, 0
                square_width, square_height = 1200, 1200

                # Redimensionar a imagem para o formato landscape, se necessário
                if landscape_width and landscape_height:
                    ratio = min(landscape_width / width, landscape_height / height)
                elif landscape_width:
                    ratio = landscape_width / width
                elif landscape_height:
                    ratio = landscape_height / height
                else:
                    ratio = 1.0

                new_width = int(width * ratio)
                new_height = int(height * ratio)

                # Redimensionar a imagem para o formato square, se necessário
                if width > square_width or height > square_height:
                    ratio = min(square_width / width, square_height / height)
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)
                    original_image = original_image.resize((new_width, new_height))


                # Converter a imagem para o formato WebP
                converted_image = original_image.convert("RGB")

                # Obter a resolução da imagem redimensionada
                resized_width, resized_height = converted_image.size
                print(f"Resolução da imagem redimensionada: {resized_width} x {resized_height} pixels")


                # Comprimir a imagem
                compressed_image = io.BytesIO()

                converted_image.save(compressed_image, format='WebP', quality=65)
                compressed_image.seek(0)

                # Obter o tamanho da imagem comprimida
                compressed_size = len(compressed_image.getvalue())
                print(f"Tamanho da imagem comprimida: {compressed_size} bytes")

                # Obter o tamanho da imagem comprimida
                compressed_size = len(compressed_image.getvalue())
                print(f"Tamanho da imagem comprimida: {compressed_size} bytes")

                # Requisição para criar a imagem destacada
                import sys
                #import time

                # Número máximo de tentativas
                max_attempts = 3
                current_attempt = 1

                while current_attempt <= max_attempts:
                    try:
                        sys.stdout.reconfigure(encoding='utf-8')
                        print(f"Tentativa {current_attempt}: Enviando imagem '{img_path[:50]}' para o WordPress...")

                        imagem_destacada_resposta = requests.post(
                            api_url + "/media",
                            auth=(usuario, senha),
                            headers=headers,
                            data=compressed_image.read()
                        )
                        imagem_destacada_resposta.raise_for_status()
                        imagem_destacada_resposta_json = imagem_destacada_resposta.json()

                        if "id" in imagem_destacada_resposta_json:
                            imagem_destacada_id = imagem_destacada_resposta_json["id"]
                            sys.stdout.reconfigure(encoding='utf-8')
                            print(f"Arquivo de imagem baixado: {imagem_destacada_nome[:50]}")
                            break  # Saia do loop se o envio foi bem-sucedido
                        else:
                            print("Erro ao criar imagem destacada")
                            print(imagem_destacada_resposta_json)
                            exit()

                    except requests.exceptions.RequestException as e:
                        print("Erro ao criar imagem destacada")
                        print(f"Exception: {e}")

                    # Atraso de 1 minuto antes de tentar novamente
                    time.sleep(60)
                    current_attempt += 1

                if current_attempt > max_attempts:
                    print(f"Falha após {max_attempts} tentativas. Verifique a conexão e tente novamente mais tarde.")
                    #exit()
                else:
                    # Caso não haja imagem em destaque, definir imagem_destacada_id como None
                    imagem_destacada_id = None


            # Requisição POST para criar novo post no blog WordPress
            headers = {"Content-Type": "application/json"}
            categoria_id = categorias_por_url[url]
            if imagem_destacada_id:
                dados = {
                    "title": new_title,
                    "content": new_content,
                    "featured_media": imagem_destacada_id,
                    "status": "publish",
                    "comment_status": "closed",
                    "ping_status": "closed",
                    "categories": [categoria_id],
                }
            else:
                dados = {
                    "title": new_title,
                    "content": new_content,
                    "status": "draft",
                    "comment_status": "closed",
                    "ping_status": "closed",
                    "categories": [categoria_id],
                    "featured_media": 0
                }
            post_resposta = requests.post(api_url + "/posts?_embed", auth=(usuario, senha), headers=headers, json=dados)

            # Verificar se a requisição foi bem-sucedida
            if post_resposta.status_code == 201:
                print("Artigo enviado com sucesso ao blog!")
            else:
                print(f"Erro ao enviar o artigo: {post_resposta.content}")

            # Adicionar item processado à lista de dados
            data.append({
                'title': title,
                'link': link,
                'processed_at': str(datetime.now())
            })

            # Salvar dados no arquivo JSON
            save_data(data)
